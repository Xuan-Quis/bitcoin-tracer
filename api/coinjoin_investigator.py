"""
CoinJoin Investigator - ƒêi·ªÅu tra s√¢u c√°c giao d·ªãch CoinJoin v·ªõi DFS
"""

import asyncio
import aiohttp
from typing import Dict, List, Set, Optional, Tuple
from datetime import datetime
import logging

from api.blockchain_api import BlockstreamAPI
from api.neo4j_storage import Neo4jStorage
from utils.config import Config
from utils.logger import get_logger

logger = get_logger(__name__)

class CoinJoinInvestigator:
    """
    ƒêi·ªÅu tra s√¢u c√°c giao d·ªãch CoinJoin v·ªõi thu·∫≠t to√°n DFS
    """
    
    def __init__(self, config: Config):
        self.config = config
        self.blockstream_api = BlockstreamAPI(config)
        self.neo4j_storage = Neo4jStorage(config)
        
        # DFS parameters
        self.max_depth = config.get('investigation_max_depth', 3)
        self.max_transactions_per_address = config.get('max_transactions_per_address', 5)
        self.max_addresses_per_tx = config.get('max_addresses_per_tx', 10)
        self.consecutive_normal_limit = config.get('consecutive_normal_limit', 10)
        self.max_non_cluster_steps = config.get('max_non_cluster_steps', 5)
        
        # Tracking
        self.visited_addresses = set()
        self.visited_transactions = set()
        self.coinjoin_addresses = set()
        self.coinjoin_transactions = set()
        self.original_input_addresses = set()
        self.start_address = None
        
    async def investigate_coinjoin(self, txid: str, tx_data: Dict, coinjoin_analysis: Dict):
        """ƒêi·ªÅu tra s√¢u m·ªôt giao d·ªãch CoinJoin"""
        logger.info(f"üîç B·∫Øt ƒë·∫ßu ƒëi·ªÅu tra CoinJoin: {txid}")
        
        try:
            # Initialize investigation
            self.visited_addresses.clear()
            self.visited_transactions.clear()
            self.coinjoin_addresses.clear()
            self.coinjoin_transactions.clear()
            
            # Extract addresses from CoinJoin transaction
            addresses = self.extract_addresses_from_transaction(tx_data)
            # Set original input cluster and start address (only 1 address as requested)
            self.original_input_addresses = {
                vin['prevout']['scriptpubkey_address']
                for vin in tx_data.get('vin', [])
                if 'prevout' in vin and 'scriptpubkey_address' in vin['prevout']
            }
            self.start_address = next(iter(self.original_input_addresses), None)
            if not self.start_address:
                # fallback to first output
                for vout in tx_data.get('vout', []):
                    if 'scriptpubkey_address' in vout:
                        self.start_address = vout['scriptpubkey_address']
                        break
            if not self.start_address:
                logger.error("Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c ƒë·ªãa ch·ªâ b·∫Øt ƒë·∫ßu")
                return
            
            # Add to CoinJoin sets
            self.coinjoin_addresses.update(addresses)
            self.coinjoin_transactions.add(txid)
            
            # Start linear investigation from single start address
            investigation_results = await self.linear_investigation(
                current_address=self.start_address,
                depth=0,
                non_cluster_steps=0
            )
            
            # Store results to Neo4j
            await self.store_investigation_results(
                txid, 
                tx_data, 
                coinjoin_analysis, 
                investigation_results
            )
            
            logger.info(f"‚úÖ Ho√†n th√†nh ƒëi·ªÅu tra CoinJoin: {txid}")
            logger.info(f"   - ƒê·ªãa ch·ªâ CoinJoin: {len(self.coinjoin_addresses)}")
            logger.info(f"   - Giao d·ªãch CoinJoin: {len(self.coinjoin_transactions)}")
            logger.info(f"   - ƒê·ªãa ch·ªâ li√™n quan: {len(self.visited_addresses)}")
            
        except Exception as e:
            logger.error(f"Error investigating CoinJoin {txid}: {e}")

    async def investigate_from_address(self, address: str, max_depth: int | None = None) -> Dict:
        """ƒêi·ªÅu tra tuy·∫øn t√≠nh b·∫Øt ƒë·∫ßu t·ª´ m·ªôt ƒë·ªãa ch·ªâ duy nh·∫•t (kh√¥ng c·∫ßn coinjoin tr∆∞·ªõc).
        - Cluster ch·ªâ ƒëi·ªÉm ƒë·∫ßu (address) v√† ƒëi·ªÉm cu·ªëi theo t·ª´ng b∆∞·ªõc.
        - D·ª´ng n·∫øu quay v·ªÅ ƒëi·ªÉm ƒë·∫ßu ho·∫∑c sau 5 giao d·ªãch m√† kh√¥ng ch·∫°m c·ª•m input.
        Tr·∫£ v·ªÅ dict k·∫øt qu·∫£ (kh√¥ng ghi Neo4j).
        """
        # Reset state
        self.visited_addresses.clear()
        self.visited_transactions.clear()
        self.coinjoin_addresses.clear()
        self.coinjoin_transactions.clear()
        self.original_input_addresses = {address}
        self.start_address = address
        original_max_depth = self.max_depth
        if isinstance(max_depth, int) and max_depth > 0:
            self.max_depth = max_depth

        try:
            results = await self.linear_investigation(address, depth=0, non_cluster_steps=0)
            return results
        finally:
            # restore
            self.max_depth = original_max_depth

    async def linear_investigation(self, current_address: str, depth: int, non_cluster_steps: int) -> Dict:
        """ƒêi·ªÅu tra tuy·∫øn t√≠nh theo 1 ƒë·ªãa ch·ªâ, ch·ªâ cluster ƒëi·ªÉm ƒë·∫ßu/cu·ªëi.
        D·ª´ng n·∫øu ƒëi·ªÉm cu·ªëi tr√πng ƒëi·ªÉm ƒë·∫ßu, ho·∫∑c sau 5 tx kh√¥ng t√¨m th·∫•y cluster v·ªõi input cluster.
        """
        if depth >= self.max_depth:
            return {
                'depth': depth,
                'addresses_processed': len(self.visited_addresses),
                'coinjoin_found': len(self.coinjoin_transactions),
                'normal_found': 0,
                'related_addresses': set(),
                'related_transactions': set()
            }

        self.visited_addresses.add(current_address)

        # Fetch transactions of current address (limit)
        address_txs = await self.fetch_address_transactions(current_address)
        address_txs = (address_txs or [])[: self.max_transactions_per_address]

        results = {
            'depth': depth,
            'addresses_processed': len(self.visited_addresses),
            'coinjoin_found': 0,
            'normal_found': 0,
            'related_addresses': set(),
            'related_transactions': set()
        }

        for tx in address_txs:
            txid = tx.get('txid')
            if not txid or txid in self.visited_transactions:
                continue
            self.visited_transactions.add(txid)

            tx_addresses = self.extract_addresses_from_transaction(tx)
            results['related_addresses'].update(tx_addresses)
            results['related_transactions'].add(txid)

            # Check loop closure: end matches start
            if self.start_address in tx_addresses and depth > 0:
                logger.info(f"üîÅ ƒêi·ªÉm cu·ªëi tr√πng ƒëi·ªÉm ƒë·∫ßu t·∫°i tx {txid}, d·ª´ng ƒëi·ªÅu tra")
                return results

            # Analyze coinjoin
            coinjoin_analysis = await self.analyze_transaction_coinjoin(tx)
            if coinjoin_analysis.get('is_coinjoin', False):
                self.coinjoin_transactions.add(txid)
                self.coinjoin_addresses.update(tx_addresses)
                results['coinjoin_found'] += 1

            # Cluster match with original input cluster?
            cluster_match_found = bool(tx_addresses & self.original_input_addresses)
            if cluster_match_found:
                non_cluster_steps = 0
            else:
                non_cluster_steps += 1
                if non_cluster_steps >= self.max_non_cluster_steps:
                    logger.info("‚õî Kh√¥ng t√¨m th·∫•y cluster v·ªõi input sau 5 tx, d·ª´ng ƒëi·ªÅu tra")
                    return results

            # Choose next end address (first address not equal current)
            next_address = None
            for addr in tx_addresses:
                if addr != current_address:
                    next_address = addr
                    break
            if not next_address:
                continue

            # Recurse linearly
            sub = await self.linear_investigation(next_address, depth + 1, non_cluster_steps)
            # Merge minimal counters
            results['addresses_processed'] = max(results['addresses_processed'], sub.get('addresses_processed', 0))
            results['coinjoin_found'] += sub.get('coinjoin_found', 0)
            results['normal_found'] += sub.get('normal_found', 0)
            results['related_addresses'].update(sub.get('related_addresses', set()))
            results['related_transactions'].update(sub.get('related_transactions', set()))

            # After first successful walk, stop (single-path)
            break

        return results
    
    def extract_addresses_from_transaction(self, tx_data: Dict) -> Set[str]:
        """Tr√≠ch xu·∫•t t·∫•t c·∫£ ƒë·ªãa ch·ªâ t·ª´ transaction"""
        addresses = set()
        
        # Input addresses
        for vin in tx_data.get('vin', []):
            if 'prevout' in vin and 'scriptpubkey_address' in vin['prevout']:
                addresses.add(vin['prevout']['scriptpubkey_address'])
        
        # Output addresses
        for vout in tx_data.get('vout', []):
            if 'scriptpubkey_address' in vout:
                addresses.add(vout['scriptpubkey_address'])
        
        return addresses
    
    async def dfs_investigation(
        self, 
        addresses: Set[str], 
        depth: int, 
        consecutive_normal: int
    ) -> Dict:
        """DFS investigation cho c√°c ƒë·ªãa ch·ªâ"""
        
        if depth >= self.max_depth:
            logger.debug(f"ƒê·∫°t ƒë·ªô s√¢u t·ªëi ƒëa: {depth}")
            return {}
        
        if consecutive_normal >= self.consecutive_normal_limit:
            logger.debug(f"G·∫∑p qu√° nhi·ªÅu giao d·ªãch normal li√™n ti·∫øp: {consecutive_normal}")
            return {}
        
        investigation_results = {
            'depth': depth,
            'addresses_processed': len(addresses),
            'coinjoin_found': 0,
            'normal_found': 0,
            'related_addresses': set(),
            'related_transactions': set()
        }
        
        for address in addresses:
            if address in self.visited_addresses:
                continue
                
            self.visited_addresses.add(address)
            
            # Fetch address transactions
            address_txs = await self.fetch_address_transactions(address)
            if not address_txs:
                continue
            
            # Limit transactions per address
            address_txs = address_txs[:self.max_transactions_per_address]
            
            local_consecutive_normal = consecutive_normal
            
            for tx in address_txs:
                txid = tx.get('txid')
                if not txid or txid in self.visited_transactions:
                    continue
                
                self.visited_transactions.add(txid)
                
                # Analyze transaction for CoinJoin
                coinjoin_analysis = await self.analyze_transaction_coinjoin(tx)
                
                if coinjoin_analysis.get('is_coinjoin', False):
                    logger.info(f"üîç Ph√°t hi·ªán CoinJoin li√™n quan: {txid} (depth: {depth})")
                    
                    # Add to CoinJoin sets
                    self.coinjoin_transactions.add(txid)
                    tx_addresses = self.extract_addresses_from_transaction(tx)
                    self.coinjoin_addresses.update(tx_addresses)
                    investigation_results['related_addresses'].update(tx_addresses)
                    investigation_results['related_transactions'].add(txid)
                    investigation_results['coinjoin_found'] += 1
                    
                    # Reset consecutive normal counter
                    local_consecutive_normal = 0
                    
                    # Continue DFS for this CoinJoin
                    if depth < self.max_depth - 1:
                        await self.dfs_investigation(
                            tx_addresses, 
                            depth + 1, 
                            local_consecutive_normal
                        )
                else:
                    local_consecutive_normal += 1
                    investigation_results['normal_found'] += 1
                    
                    # Add addresses from normal transaction (limited)
                    tx_addresses = self.extract_addresses_from_transaction(tx)
                    limited_addresses = set(list(tx_addresses)[:self.max_addresses_per_tx])
                    investigation_results['related_addresses'].update(limited_addresses)
                    investigation_results['related_transactions'].add(txid)
        
        return investigation_results
    
    async def fetch_address_transactions(self, address: str) -> List[Dict]:
        """Fetch transactions c·ªßa m·ªôt ƒë·ªãa ch·ªâ"""
        try:
            url = f"https://blockstream.info/api/address/{address}/txs"
            async with aiohttp.ClientSession() as session:
                async with session.get(url) as response:
                    if response.status == 200:
                        return await response.json()
                    return []
        except Exception as e:
            logger.error(f"Error fetching transactions for {address}: {e}")
            return []
    
    async def analyze_transaction_coinjoin(self, tx_data: Dict) -> Dict:
        """Ph√¢n t√≠ch m·ªôt transaction c√≥ ph·∫£i CoinJoin kh√¥ng (heuristic)"""
        from api.detector_adapter import detect_coinjoin
        return detect_coinjoin(tx_data)
    
    # --- Tree-building investigation (unified for tx/address) ---
    async def fetch_transaction_details_async(self, txid: str) -> Optional[Dict]:
        """Fetch chi ti·∫øt transaction (async)."""
        try:
            url = f"https://blockstream.info/api/tx/{txid}"
            async with aiohttp.ClientSession() as session:
                async with session.get(url) as response:
                    if response.status == 200:
                        return await response.json()
                    return None
        except Exception as e:
            logger.error(f"Error fetching transaction {txid}: {e}")
            return None

    async def build_tree_from_txid(self, txid: str, max_depth: int = 10) -> Dict:
        """X√¢y d·ª±ng c√¢y giao d·ªãch b·∫Øt ƒë·∫ßu t·ª´ 1 txid.
        D·ª´ng khi c·ª•m ƒë·∫ßu ra ch·∫°m c·ª•m input ban ƒë·∫ßu ho·∫∑c ƒë·∫°t depth = max_depth (t·ªëi ƒëa 10).
        """
        self.visited_transactions.clear()
        self.visited_addresses.clear()
        self.coinjoin_addresses.clear()
        self.coinjoin_transactions.clear()

        self.max_depth = min(int(max_depth or 10), 10)

        root_tx = await self.fetch_transaction_details_async(txid)
        if not root_tx:
            return {}

        # Original input cluster: all input addresses of root tx
        self.original_input_addresses = {
            vin.get('prevout', {}).get('scriptpubkey_address')
            for vin in root_tx.get('vin', [])
            if vin.get('prevout', {}).get('scriptpubkey_address')
        }

        return await self._build_tree_recursive(root_tx, depth=0)

    async def build_tree_from_address(self, address: str, max_depth: int = 10) -> Dict:
        """X√¢y d·ª±ng c√¢y giao d·ªãch b·∫Øt ƒë·∫ßu t·ª´ m·ªôt ƒë·ªãa ch·ªâ.
        Ch·ªçn transaction ƒë·∫ßu ti√™n m√† ƒë·ªãa ch·ªâ xu·∫•t hi·ªán ·ªü input, n·∫øu kh√¥ng c√≥ th√¨ d√πng transaction ƒë·∫ßu ti√™n.
        """
        self.visited_transactions.clear()
        self.visited_addresses.clear()
        self.coinjoin_addresses.clear()
        self.coinjoin_transactions.clear()

        self.max_depth = min(int(max_depth or 10), 10)
        self.original_input_addresses = {address}

        txs = await self.fetch_address_transactions(address)
        if not txs:
            return {}

        # Prefer a tx where address appears in input
        start_tx = None
        for tx in txs:
            for vin in tx.get('vin', []) or []:
                if vin.get('prevout', {}).get('scriptpubkey_address') == address:
                    start_tx = tx
                    break
            if start_tx:
                break
        if start_tx is None:
            start_tx = txs[0]

        # Ensure we have full details (address list call may already be detailed, but normalize)
        txid = start_tx.get('txid') or start_tx.get('hash')
        if not txid:
            return {}
        full_tx = await self.fetch_transaction_details_async(txid)
        if not full_tx:
            # fallback to what we have
            full_tx = start_tx

        return await self._build_tree_recursive(full_tx, depth=0)

    async def _build_tree_recursive(self, tx_data: Dict, depth: int) -> Dict:
        """ƒê·ªá quy x√¢y c√¢y giao d·ªãch theo d·∫°ng:
        { tx: {...}, out: [ { tx: {...}, out: [...] }, ... ] }
        """
        if depth >= self.max_depth:
            return { 'tx': self._compact_tx(tx_data), 'out': [] }

        txid = tx_data.get('txid') or tx_data.get('hash')
        if not txid:
            return { 'tx': self._compact_tx(tx_data), 'out': [] }

        if txid in self.visited_transactions:
            return { 'tx': self._compact_tx(tx_data), 'out': [] }
        self.visited_transactions.add(txid)

        # Collect child transactions per output address
        child_nodes = []
        out_addresses = [v.get('scriptpubkey_address') for v in tx_data.get('vout', []) if v.get('scriptpubkey_address')]

        for addr in out_addresses:
            # If output cluster intersects original input cluster, stop here
            if addr in self.original_input_addresses and depth > 0:
                # closure condition reached
                return { 'tx': self._compact_tx(tx_data), 'out': [] }

            # Find child txs that spend from this address
            address_txs = await self.fetch_address_transactions(addr)
            # Filter txs where addr appears in inputs (spent by)
            child_txids = []
            for t in address_txs or []:
                t_txid = t.get('txid') or t.get('hash')
                if not t_txid or t_txid == txid:
                    continue
                vins = t.get('vin', []) or []
                if any(v.get('prevout', {}).get('scriptpubkey_address') == addr for v in vins):
                    child_txids.append(t_txid)

            # For each child, recurse
            for c_txid in child_txids:
                child_full = await self.fetch_transaction_details_async(c_txid)
                if not child_full:
                    continue

                # Stop if child's outputs intersect original input cluster
                child_out_addrs = {
                    v.get('scriptpubkey_address') for v in child_full.get('vout', []) if v.get('scriptpubkey_address')
                }
                if child_out_addrs & self.original_input_addresses:
                    child_nodes.append({ 'tx': self._compact_tx(child_full), 'out': [] })
                    # Do not expand further on closure
                    continue

                subtree = await self._build_tree_recursive(child_full, depth + 1)
                child_nodes.append(subtree)

        return { 'tx': self._compact_tx(tx_data), 'out': child_nodes }

    def _compact_tx(self, tx_data: Dict) -> Dict:
        """R√∫t g·ªçn th√¥ng tin tx ƒë·ªÉ hi·ªÉn th·ªã trong c√¢y."""
        return {
            'txid': tx_data.get('txid') or tx_data.get('hash'),
            'vin_count': len(tx_data.get('vin', []) or []),
            'vout_count': len(tx_data.get('vout', []) or []),
            'fee': tx_data.get('fee'),
            'size': tx_data.get('size'),
        }

    async def store_investigation_results(
        self, 
        original_txid: str, 
        original_tx_data: Dict, 
        coinjoin_analysis: Dict, 
        investigation_results: Dict
    ):
        """L∆∞u k·∫øt qu·∫£ ƒëi·ªÅu tra v√†o Neo4j"""
        try:
            # Prepare data for Neo4j
            neo4j_data = {
                'original_transaction': {
                    'txid': original_txid,
                    'timestamp': datetime.now().isoformat(),
                    'coinjoin_score': coinjoin_analysis.get('coinjoin_score', 0),
                    'detection_method': coinjoin_analysis.get('detection_method', 'unknown'),
                    'indicators': coinjoin_analysis.get('indicators', {}),
                    'fee': original_tx_data.get('fee', 0),
                    'size': original_tx_data.get('size', 0)
                },
                'coinjoin_addresses': list(self.coinjoin_addresses),
                'coinjoin_transactions': list(self.coinjoin_transactions),
                'related_addresses': list(investigation_results.get('related_addresses', set())),
                'related_transactions': list(investigation_results.get('related_transactions', set())),
                'investigation_stats': {
                    'depth_reached': investigation_results.get('depth', 0),
                    'addresses_processed': investigation_results.get('addresses_processed', 0),
                    'coinjoin_found': investigation_results.get('coinjoin_found', 0),
                    'normal_found': investigation_results.get('normal_found', 0)
                }
            }
            
            # Store to Neo4j
            await self.neo4j_storage.store_coinjoin_investigation(neo4j_data)
            
            logger.info(f"üíæ ƒê√£ l∆∞u k·∫øt qu·∫£ ƒëi·ªÅu tra v√†o Neo4j: {original_txid}")
            
        except Exception as e:
            logger.error(f"Error storing investigation results: {e}")
