"""
CoinJoin Investigator - ƒêi·ªÅu tra s√¢u c√°c giao d·ªãch CoinJoin v·ªõi DFS
"""

import asyncio
import aiohttp
from typing import Dict, List, Set, Optional, Tuple
from datetime import datetime
import logging
import time # Added for time.time()

from api.blockchain_api import BlockstreamAPI
from api.neo4j_storage import Neo4jStorage
from utils.config import Config
from utils.logger import get_logger
from utils.cache import transaction_cache  # T·ªêI ∆ØU: S·ª≠ d·ª•ng global cache

logger = get_logger(__name__)

class CoinJoinInvestigator:
    """
    ƒêi·ªÅu tra s√¢u c√°c giao d·ªãch CoinJoin v·ªõi thu·∫≠t to√°n DFS
    """
    
    def __init__(self, config: Config):
        self.config = config
        self.blockstream_api = BlockstreamAPI(config)
        self.neo4j_storage = Neo4jStorage(config)
        
        # DFS parameters - T·ªêI ∆ØU: ƒêi·ªÅu ch·ªânh ƒë·ªÉ truy v·∫øt s√¢u h∆°n
        self.max_depth = config.get('investigation_max_depth', 10)  # TƒÉng t·ª´ 6 l√™n 10
        self.max_transactions_per_address = config.get('max_transactions_per_address', 15)  # TƒÉng t·ª´ 8 l√™n 15
        self.max_addresses_per_tx = config.get('max_addresses_per_tx', 25)  # TƒÉng t·ª´ 15 l√™n 25
        self.consecutive_normal_limit = config.get('consecutive_normal_limit', 10)  # TƒÉng t·ª´ 8 l√™n 10
        self.max_non_cluster_steps = config.get('max_non_cluster_steps', 5)  # TƒÉng t·ª´ 3 l√™n 5
        
        # T·ªêI ∆ØU: ƒêi·ªÅu ch·ªânh ƒë·ªÉ truy v·∫øt s√¢u h∆°n
        self.max_branches_per_node = config.get('max_branches_per_node', 5)  # TƒÉng t·ª´ 2 l√™n 5
        self.min_heuristic_score = config.get('min_heuristic_score', 0.2)  # Gi·∫£m t·ª´ 0.3 xu·ªëng 0.2
        self.max_exchange_like_score = config.get('max_exchange_like_score', 0.7)  # TƒÉng t·ª´ 0.4 l√™n 0.7
        
        # T·ªêI ∆ØU M·ªöI: Th√™m c∆° ch·∫ø d·ª´ng th√¥ng minh
        self.max_total_nodes = config.get('max_total_nodes', 1000)  # Gi·ªõi h·∫°n t·ªïng s·ªë nodes
        self.max_time_seconds = config.get('max_time_seconds', 60)  # Gi·ªõi h·∫°n th·ªùi gian t·ªëi ƒëa
        self.min_coinjoin_ratio = config.get('min_coinjoin_ratio', 0.1)  # T·ª∑ l·ªá CoinJoin t·ªëi thi·ªÉu ƒë·ªÉ ti·∫øp t·ª•c
        
        # Tracking
        self.visited_addresses = set()
        self.visited_transactions = set()
        self.coinjoin_addresses = set()
        self.coinjoin_transactions = set()
        self.original_input_addresses = set()
        self.start_address = None
        
        # T·ªêI ∆ØU M·ªöI: Tracking cho performance monitoring
        self.total_nodes_processed = 0
        self.start_time = None
        self.should_stop_early = False
        
        # T·ªêI ∆ØU: S·ª≠ d·ª•ng global cache thay v√¨ local cache
        
    def _should_stop_early(self) -> bool:
        """T·ªêI ∆ØU M·ªöI: Ki·ªÉm tra c√≥ n√™n d·ª´ng s·ªõm kh√¥ng d·ª±a tr√™n performance metrics"""
        if self.should_stop_early:
            return True
            
        # Ki·ªÉm tra s·ªë l∆∞·ª£ng nodes ƒë√£ x·ª≠ l√Ω
        if self.total_nodes_processed >= self.max_total_nodes:
            logger.info(f"üõë D·ª´ng s·ªõm: ƒê√£ x·ª≠ l√Ω {self.total_nodes_processed} nodes (gi·ªõi h·∫°n: {self.max_total_nodes})")
            return True
            
        # Ki·ªÉm tra th·ªùi gian
        if self.start_time:
            elapsed_time = time.time() - self.start_time
            if elapsed_time >= self.max_time_seconds:
                logger.info(f"üõë D·ª´ng s·ªõm: ƒê√£ m·∫•t {elapsed_time:.1f}s (gi·ªõi h·∫°n: {self.max_time_seconds}s)")
                return True
                
        # Ki·ªÉm tra t·ª∑ l·ªá CoinJoin
        if self.total_nodes_processed > 100:
            coinjoin_ratio = len(self.coinjoin_transactions) / self.total_nodes_processed
            if coinjoin_ratio < self.min_coinjoin_ratio:
                logger.info(f"üõë D·ª´ng s·ªõm: T·ª∑ l·ªá CoinJoin qu√° th·∫•p ({coinjoin_ratio:.2%})")
                return True
                
        return False

    async def investigate_coinjoin(self, txid: str, tx_data: Dict, coinjoin_analysis: Dict):
        """ƒêi·ªÅu tra s√¢u m·ªôt giao d·ªãch CoinJoin"""
        logger.info(f"üîç B·∫Øt ƒë·∫ßu ƒëi·ªÅu tra CoinJoin: {txid}")
        
        try:
            # Initialize investigation
            self.visited_addresses.clear()
            self.visited_transactions.clear()
            self.coinjoin_addresses.clear()
            self.coinjoin_transactions.clear()
            # T·ªêI ∆ØU: Clear cache m·ªói l·∫ßn investigate m·ªõi
            transaction_cache.clear()
            
            # T·ªêI ∆ØU M·ªöI: Kh·ªüi t·∫°o performance tracking
            self.total_nodes_processed = 0
            self.start_time = time.time()
            self.should_stop_early = False
            
            # Extract addresses from CoinJoin transaction
            addresses = self.extract_addresses_from_transaction(tx_data)
            # Set original input cluster and start address (only 1 address as requested)
            self.original_input_addresses = {
                vin['prevout']['scriptpubkey_address']
                for vin in tx_data.get('vin', [])
                if 'prevout' in vin and 'scriptpubkey_address' in vin['prevout']
            }
            self.start_address = next(iter(self.original_input_addresses), None)
            if not self.start_address:
                # fallback to first output
                for vout in tx_data.get('vout', []):
                    if 'scriptpubkey_address' in vout:
                        self.start_address = vout['scriptpubkey_address']
                        break
            if not self.start_address:
                logger.error("Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c ƒë·ªãa ch·ªâ b·∫Øt ƒë·∫ßu")
                return
            
            # Add to CoinJoin sets
            self.coinjoin_addresses.update(addresses)
            self.coinjoin_transactions.add(txid)
            
            # Start linear investigation from single start address
            investigation_results = await self.linear_investigation(
                current_address=self.start_address,
                depth=0,
                non_cluster_steps=0
            )
            
            # Store results to Neo4j
            await self.store_investigation_results(
                txid, 
                tx_data, 
                coinjoin_analysis, 
                investigation_results
            )
            
            logger.info(f"‚úÖ Ho√†n th√†nh ƒëi·ªÅu tra CoinJoin: {txid}")
            logger.info(f"   - ƒê·ªãa ch·ªâ CoinJoin: {len(self.coinjoin_addresses)}")
            logger.info(f"   - Giao d·ªãch CoinJoin: {len(self.coinjoin_transactions)}")
            logger.info(f"   - ƒê·ªãa ch·ªâ li√™n quan: {len(self.visited_addresses)}")
            
        except Exception as e:
            logger.error(f"Error investigating CoinJoin {txid}: {e}")

    async def investigate_from_address(self, address: str, max_depth: int | None = None) -> Dict:
        """ƒêi·ªÅu tra tuy·∫øn t√≠nh b·∫Øt ƒë·∫ßu t·ª´ m·ªôt ƒë·ªãa ch·ªâ duy nh·∫•t (kh√¥ng c·∫ßn coinjoin tr∆∞·ªõc).
        - Cluster ch·ªâ ƒëi·ªÉm ƒë·∫ßu (address) v√† ƒëi·ªÉm cu·ªëi theo t·ª´ng b∆∞·ªõc.
        - D·ª´ng n·∫øu quay v·ªÅ ƒëi·ªÉm ƒë·∫ßu ho·∫∑c sau 5 giao d·ªãch m√† kh√¥ng ch·∫°m c·ª•m input.
        Tr·∫£ v·ªÅ dict k·∫øt qu·∫£ (kh√¥ng ghi Neo4j).
        """
        # Reset state
        self.visited_addresses.clear()
        self.visited_transactions.clear()
        self.coinjoin_addresses.clear()
        self.coinjoin_transactions.clear()
        self.original_input_addresses = {address}
        self.start_address = address
        original_max_depth = self.max_depth
        if isinstance(max_depth, int) and max_depth > 0:
            self.max_depth = max_depth

        try:
            results = await self.linear_investigation(address, depth=0, non_cluster_steps=0)
            return results
        finally:
            # restore
            self.max_depth = original_max_depth

    async def linear_investigation(self, current_address: str, depth: int, non_cluster_steps: int) -> Dict:
        """ƒêi·ªÅu tra tuy·∫øn t√≠nh theo 1 ƒë·ªãa ch·ªâ, ch·ªâ cluster ƒëi·ªÉm ƒë·∫ßu/cu·ªëi.
        D·ª´ng n·∫øu ƒëi·ªÉm cu·ªëi tr√πng ƒëi·ªÉm ƒë·∫ßu, ho·∫∑c sau 5 tx kh√¥ng t√¨m th·∫•y cluster v·ªõi input cluster.
        """
        if depth >= self.max_depth:
            return {
                'depth': depth,
                'addresses_processed': len(self.visited_addresses),
                'coinjoin_found': len(self.coinjoin_transactions),
                'normal_found': 0,
                'related_addresses': set(),
                'related_transactions': set()
            }

        self.visited_addresses.add(current_address)

        # Fetch transactions of current address (limit)
        address_txs = await self.fetch_address_transactions(current_address)
        address_txs = (address_txs or [])[: self.max_transactions_per_address]

        results = {
            'depth': depth,
            'addresses_processed': len(self.visited_addresses),
            'coinjoin_found': 0,
            'normal_found': 0,
            'related_addresses': set(),
            'related_transactions': set()
        }

        for tx in address_txs:
            txid = tx.get('txid')
            if not txid or txid in self.visited_transactions:
                continue
            self.visited_transactions.add(txid)

            tx_addresses = self.extract_addresses_from_transaction(tx)
            results['related_addresses'].update(tx_addresses)
            results['related_transactions'].add(txid)

            # Check loop closure: end matches start
            if self.start_address in tx_addresses and depth > 0:
                logger.info(f"üîÅ ƒêi·ªÉm cu·ªëi tr√πng ƒëi·ªÉm ƒë·∫ßu t·∫°i tx {txid}, d·ª´ng ƒëi·ªÅu tra")
                return results

            # Analyze coinjoin
            coinjoin_analysis = await self.analyze_transaction_coinjoin(tx)
            if coinjoin_analysis.get('is_coinjoin', False):
                self.coinjoin_transactions.add(txid)
                self.coinjoin_addresses.update(tx_addresses)
                results['coinjoin_found'] += 1

            # Cluster match with original input cluster?
            cluster_match_found = bool(tx_addresses & self.original_input_addresses)
            if cluster_match_found:
                non_cluster_steps = 0
            else:
                non_cluster_steps += 1
                if non_cluster_steps >= self.max_non_cluster_steps:
                    logger.info("‚õî Kh√¥ng t√¨m th·∫•y cluster v·ªõi input sau 5 tx, d·ª´ng ƒëi·ªÅu tra")
                    return results

            # Choose next end address (first address not equal current)
            next_address = None
            for addr in tx_addresses:
                if addr != current_address:
                    next_address = addr
                    break
            if not next_address:
                continue

            # Recurse linearly
            sub = await self.linear_investigation(next_address, depth + 1, non_cluster_steps)
            # Merge minimal counters
            results['addresses_processed'] = max(results['addresses_processed'], sub.get('addresses_processed', 0))
            results['coinjoin_found'] += sub.get('coinjoin_found', 0)
            results['normal_found'] += sub.get('normal_found', 0)
            results['related_addresses'].update(sub.get('related_addresses', set()))
            results['related_transactions'].update(sub.get('related_transactions', set()))

            # After first successful walk, stop (single-path)
            break

        return results
    
    def extract_addresses_from_transaction(self, tx_data: Dict) -> Set[str]:
        """Tr√≠ch xu·∫•t t·∫•t c·∫£ ƒë·ªãa ch·ªâ t·ª´ transaction"""
        addresses = set()
        
        # Input addresses
        for vin in tx_data.get('vin', []):
            if 'prevout' in vin and 'scriptpubkey_address' in vin['prevout']:
                addresses.add(vin['prevout']['scriptpubkey_address'])
        
        # Output addresses
        for vout in tx_data.get('vout', []):
            if 'scriptpubkey_address' in vout:
                addresses.add(vout['scriptpubkey_address'])
        
        return addresses
    
    async def dfs_investigation(
        self, 
        addresses: Set[str], 
        depth: int, 
        consecutive_normal: int
    ) -> Dict:
        """DFS investigation cho c√°c ƒë·ªãa ch·ªâ"""
        
        if depth >= self.max_depth:
            logger.debug(f"ƒê·∫°t ƒë·ªô s√¢u t·ªëi ƒëa: {depth}")
            return {}
        
        if consecutive_normal >= self.consecutive_normal_limit:
            logger.debug(f"G·∫∑p qu√° nhi·ªÅu giao d·ªãch normal li√™n ti·∫øp: {consecutive_normal}")
            return {}
        
        investigation_results = {
            'depth': depth,
            'addresses_processed': len(addresses),
            'coinjoin_found': 0,
            'normal_found': 0,
            'related_addresses': set(),
            'related_transactions': set()
        }
        
        for address in addresses:
            if address in self.visited_addresses:
                continue
                
            self.visited_addresses.add(address)
            
            # Fetch address transactions
            address_txs = await self.fetch_address_transactions(address)
            if not address_txs:
                continue
            
            # Limit transactions per address
            address_txs = address_txs[:self.max_transactions_per_address]
            
            local_consecutive_normal = consecutive_normal
            
            for tx in address_txs:
                txid = tx.get('txid')
                if not txid or txid in self.visited_transactions:
                    continue
                
                self.visited_transactions.add(txid)
                
                # Analyze transaction for CoinJoin
                coinjoin_analysis = await self.analyze_transaction_coinjoin(tx)
                
                if coinjoin_analysis.get('is_coinjoin', False):
                    logger.info(f"üîç Ph√°t hi·ªán CoinJoin li√™n quan: {txid} (depth: {depth})")
                    
                    # Add to CoinJoin sets
                    self.coinjoin_transactions.add(txid)
                    tx_addresses = self.extract_addresses_from_transaction(tx)
                    self.coinjoin_addresses.update(tx_addresses)
                    investigation_results['related_addresses'].update(tx_addresses)
                    investigation_results['related_transactions'].add(txid)
                    investigation_results['coinjoin_found'] += 1
                    
                    # Reset consecutive normal counter
                    local_consecutive_normal = 0
                    
                    # Continue DFS for this CoinJoin
                    if depth < self.max_depth - 1:
                        await self.dfs_investigation(
                            tx_addresses, 
                            depth + 1, 
                            local_consecutive_normal
                        )
                else:
                    local_consecutive_normal += 1
                    investigation_results['normal_found'] += 1
                    
                    # Add addresses from normal transaction (limited)
                    tx_addresses = self.extract_addresses_from_transaction(tx)
                    limited_addresses = set(list(tx_addresses)[:self.max_addresses_per_tx])
                    investigation_results['related_addresses'].update(limited_addresses)
                    investigation_results['related_transactions'].add(txid)
        
        return investigation_results
    
    async def fetch_address_transactions(self, address: str) -> List[Dict]:
        """Fetch transactions c·ªßa m·ªôt ƒë·ªãa ch·ªâ - T·ªêI ∆ØU: S·ª≠ d·ª•ng cache"""
        # T·ªêI ∆ØU: Check cache tr∆∞·ªõc
        cached_data = transaction_cache.get_address_transactions(address)
        if cached_data is not None:
            logger.debug(f"Cache hit for address {address[:10]}...")
            return cached_data
            
        try:
            url = f"https://blockstream.info/api/address/{address}/txs"
            async with aiohttp.ClientSession() as session:
                async with session.get(url) as response:
                    if response.status == 200:
                        data = await response.json()
                        # T·ªêI ∆ØU: Cache k·∫øt qu·∫£
                        transaction_cache.set_address_transactions(address, data)
                        return data
                    return []
        except Exception as e:
            logger.error(f"Error fetching transactions for {address}: {e}")
            return []
    
    async def analyze_transaction_coinjoin(self, tx_data: Dict) -> Dict:
        """Ph√¢n t√≠ch m·ªôt transaction c√≥ ph·∫£i CoinJoin kh√¥ng (heuristic)"""
        from api.detector_adapter import detect_coinjoin
        return detect_coinjoin(tx_data)
    
    # --- Tree-building investigation (unified for tx/address) ---
    async def fetch_transaction_details_async(self, txid: str) -> Optional[Dict]:
        """Fetch chi ti·∫øt transaction (async) - T·ªêI ∆ØU: S·ª≠ d·ª•ng cache"""
        # T·ªêI ∆ØU: Check cache tr∆∞·ªõc
        cached_data = transaction_cache.get_transaction(txid)
        if cached_data is not None:
            logger.debug(f"Cache hit for tx {txid[:10]}...")
            return cached_data
            
        try:
            url = f"https://blockstream.info/api/tx/{txid}"
            async with aiohttp.ClientSession() as session:
                async with session.get(url) as response:
                    if response.status == 200:
                        data = await response.json()
                        # T·ªêI ∆ØU: Cache k·∫øt qu·∫£
                        transaction_cache.set_transaction(txid, data)
                        return data
                    return None
        except Exception as e:
            logger.error(f"Error fetching transaction {txid}: {e}")
            return None

    async def build_tree_from_txid(self, txid: str, max_depth: int = 10) -> Dict:
        """X√¢y d·ª±ng c√¢y giao d·ªãch b·∫Øt ƒë·∫ßu t·ª´ 1 txid.
        D·ª´ng khi c·ª•m ƒë·∫ßu ra ch·∫°m c·ª•m input ban ƒë·∫ßu ho·∫∑c ƒë·∫°t depth = max_depth (t·ªëi ƒëa 10).
        """
        self.visited_transactions.clear()
        self.visited_addresses.clear()
        self.coinjoin_addresses.clear()
        self.coinjoin_transactions.clear()
        # T·ªêI ∆ØU: Clear cache m·ªói l·∫ßn build tree m·ªõi
        transaction_cache.clear()

        # T·ªêI ∆ØU: Gi·ªõi h·∫°n depth t·ªëi ƒëa
        self.max_depth = min(int(max_depth or 10), 10)  # Gi·ªØ nguy√™n 10 ƒë·ªÉ truy v·∫øt s√¢u

        root_tx = await self.fetch_transaction_details_async(txid)
        if not root_tx:
            return {}

        # Original input cluster: all input addresses of root tx
        self.original_input_addresses = {
            vin.get('prevout', {}).get('scriptpubkey_address')
            for vin in root_tx.get('vin', [])
            if vin.get('prevout', {}).get('scriptpubkey_address')
        }

        return await self._build_tree_recursive(root_tx, depth=0)

    async def build_tree_from_address(self, address: str, max_depth: int = 10) -> Dict:
        """X√¢y d·ª±ng c√¢y giao d·ªãch b·∫Øt ƒë·∫ßu t·ª´ m·ªôt ƒë·ªãa ch·ªâ.
        Ch·ªçn transaction ƒë·∫ßu ti√™n m√† ƒë·ªãa ch·ªâ xu·∫•t hi·ªán ·ªü input, n·∫øu kh√¥ng c√≥ th√¨ d√πng transaction ƒë·∫ßu ti√™n.
        """
        self.visited_transactions.clear()
        self.visited_addresses.clear()
        self.coinjoin_addresses.clear()
        self.coinjoin_transactions.clear()
        # T·ªêI ∆ØU: Clear cache m·ªói l·∫ßn build tree m·ªõi
        transaction_cache.clear()

        # T·ªêI ∆ØU: Gi·ªõi h·∫°n depth t·ªëi ƒëa
        self.max_depth = min(int(max_depth or 10), 10)  # Gi·ªØ nguy√™n 10 ƒë·ªÉ truy v·∫øt s√¢u
        self.original_input_addresses = {address}

        txs = await self.fetch_address_transactions(address)
        if not txs:
            return {}

        # Prefer a tx where address appears in input
        start_tx = None
        for tx in txs:
            for vin in tx.get('vin', []) or []:
                if vin.get('prevout', {}).get('scriptpubkey_address') == address:
                    start_tx = tx
                    break
            if start_tx:
                break
        if start_tx is None:
            start_tx = txs[0]

        # Ensure we have full details (address list call may already be detailed, but normalize)
        txid = start_tx.get('txid') or start_tx.get('hash')
        if not txid:
            return {}
        full_tx = await self.fetch_transaction_details_async(txid)
        if not full_tx:
            # fallback to what we have
            full_tx = start_tx

        return await self._build_tree_recursive(full_tx, depth=0)

    async def _build_tree_recursive(self, tx_data: Dict, depth: int) -> Dict:
        """ƒê·ªá quy x√¢y c√¢y giao d·ªãch theo d·∫°ng:
        { tx: {...}, out: [ { tx: {...}, out: [...] }, ... ] }
        T·ªêI ∆ØU: Th√™m heuristic ƒë·ªÉ c·∫Øt s·ªõm nh√°nh kh√¥ng c√≥ t√≠n hi·ªáu
        """
        # T·ªêI ∆ØU M·ªöI: Ki·ªÉm tra ƒëi·ªÅu ki·ªán d·ª´ng s·ªõm
        if self._should_stop_early():
            return { 'tx': self._compact_tx(tx_data), 'out': [] }
            
        if depth >= self.max_depth:
            return { 'tx': self._compact_tx(tx_data), 'out': [] }

        txid = tx_data.get('txid') or tx_data.get('hash')
        if not txid:
            return { 'tx': self._compact_tx(tx_data), 'out': [] }

        if txid in self.visited_transactions:
            return { 'tx': self._compact_tx(tx_data), 'out': [] }
        self.visited_transactions.add(txid)
        
        # T·ªêI ∆ØU M·ªöI: TƒÉng counter nodes ƒë√£ x·ª≠ l√Ω
        self.total_nodes_processed += 1

        # T·ªêI ∆ØU: Ki·ªÉm tra heuristic score ƒë·ªÉ quy·∫øt ƒë·ªãnh c√≥ m·ªü r·ªông nh√°nh kh√¥ng
        coinjoin_analysis = await self.analyze_transaction_coinjoin(tx_data)
        heuristic_score = coinjoin_analysis.get('score', 0.0)
        exchange_like_score = coinjoin_analysis.get('exchange_like_score', 0.0)
        
        # T·ªêI ∆ØU: N·ªõi l·ªèng ƒëi·ªÅu ki·ªán ƒë·ªÉ truy v·∫øt s√¢u h∆°n
        if heuristic_score < self.min_heuristic_score and depth > 4:  # TƒÉng t·ª´ 2 l√™n 4
            logger.debug(f"Stopping branch at depth {depth} due to low heuristic score: {heuristic_score}")
            return { 'tx': self._compact_tx(tx_data), 'out': [] }
        
        # T·ªêI ∆ØU: N·∫øu exchange-like score qu√° cao, d·ª´ng nh√°nh s·ªõm
        # T·ªêI ∆ØU: N·ªõi l·ªèng ƒëi·ªÅu ki·ªán ƒë·ªÉ truy v·∫øt s√¢u h∆°n
        if exchange_like_score > self.max_exchange_like_score and depth > 3:  # TƒÉng t·ª´ 1 l√™n 3
            logger.debug(f"Stopping branch at depth {depth} due to high exchange-like score: {exchange_like_score}")
            return { 'tx': self._compact_tx(tx_data), 'out': [] }
            
        # T·ªêI ∆ØU M·ªöI: Ki·ªÉm tra performance metrics tr∆∞·ªõc khi m·ªü r·ªông nh√°nh
        if depth > 2 and self.total_nodes_processed > 500:
            # ·ªû depth cao, ch·ªâ m·ªü r·ªông n·∫øu c√≥ t√≠n hi·ªáu CoinJoin m·∫°nh
            if not coinjoin_analysis.get('is_coinjoin', False) and heuristic_score < 0.5:
                logger.debug(f"Stopping branch at depth {depth} due to performance optimization")
                return { 'tx': self._compact_tx(tx_data), 'out': [] }

        # Collect child transactions per output address
        child_nodes = []
        out_addresses = [v.get('scriptpubkey_address') for v in tx_data.get('vout', []) if v.get('scriptpubkey_address')]

        # T·ªêI ∆ØU: Gi·ªõi h·∫°n s·ªë nh√°nh con m·ªói n√∫t
        selected_addresses = out_addresses[:self.max_branches_per_node]

        for addr in selected_addresses:
            # If output cluster intersects original input cluster, stop here
            if addr in self.original_input_addresses and depth > 0:
                # closure condition reached
                return { 'tx': self._compact_tx(tx_data), 'out': [] }

            # Find child txs that spend from this address
            address_txs = await self.fetch_address_transactions(addr)
            # Filter txs where addr appears in inputs (spent by)
            child_txids = []
            for t in address_txs or []:
                t_txid = t.get('txid') or t.get('hash')
                if not t_txid or t_txid == txid:
                    continue
                vins = t.get('vin', []) or []
                if any(v.get('prevout', {}).get('scriptpubkey_address') == addr for v in vins):
                    child_txids.append(t_txid)

            # T·ªêI ∆ØU: Gi·ªõi h·∫°n s·ªë child transactions ƒë·ªÉ tr√°nh nh√°nh qu√° r·ªông
            child_txids = child_txids[:5]  # TƒÉng t·ª´ 3 l√™n 5 ƒë·ªÉ m·ªü r·ªông nh√°nh

            # For each child, recurse
            for c_txid in child_txids:
                child_full = await self.fetch_transaction_details_async(c_txid)
                if not child_full:
                    continue

                # Stop if child's outputs intersect original input cluster
                child_out_addrs = {
                    v.get('scriptpubkey_address') for v in child_full.get('vout', []) if v.get('scriptpubkey_address')
                }
                if child_out_addrs & self.original_input_addresses:
                    child_nodes.append({ 'tx': self._compact_tx(child_full), 'out': [] })
                    # Do not expand further on closure
                    continue

                # T·ªêI ∆ØU: Ki·ªÉm tra exchange-like pattern ƒë·ªÉ d·ª´ng nh√°nh
                child_analysis = await self.analyze_transaction_coinjoin(child_full)
                child_score = child_analysis.get('score', 0.0)
                child_exchange_score = child_analysis.get('exchange_like_score', 0.0)
                
                # T·ªêI ∆ØU: N·ªõi l·ªèng ƒëi·ªÅu ki·ªán ƒë·ªÉ truy v·∫øt s√¢u h∆°n
                if child_score > self.max_exchange_like_score or child_exchange_score > self.max_exchange_like_score:
                    # Ch·ªâ d·ª´ng nh√°nh n·∫øu score qu√° cao v√† ƒë√£ ƒë·ªß s√¢u
                    if depth > 5:  # Th√™m ƒëi·ªÅu ki·ªán depth ƒë·ªÉ cho ph√©p truy v·∫øt s√¢u h∆°n
                        logger.debug(f"Stopping branch due to exchange-like pattern: {c_txid}")
                        child_nodes.append({ 'tx': self._compact_tx(child_full), 'out': [] })
                        continue

                subtree = await self._build_tree_recursive(child_full, depth + 1)
                child_nodes.append(subtree)

        return { 'tx': self._compact_tx(tx_data), 'out': child_nodes }

    def _compact_tx(self, tx_data: Dict) -> Dict:
        """R√∫t g·ªçn th√¥ng tin tx ƒë·ªÉ hi·ªÉn th·ªã trong c√¢y."""
        return {
            'txid': tx_data.get('txid') or tx_data.get('hash'),
            'vin_count': len(tx_data.get('vin', []) or []),
            'vout_count': len(tx_data.get('vout', []) or []),
            'fee': tx_data.get('fee'),
            'size': tx_data.get('size'),
        }

    async def store_investigation_results(
        self, 
        original_txid: str, 
        original_tx_data: Dict, 
        coinjoin_analysis: Dict, 
        investigation_results: Dict
    ):
        """L∆∞u k·∫øt qu·∫£ ƒëi·ªÅu tra v√†o Neo4j"""
        try:
            # Prepare data for Neo4j
            neo4j_data = {
                'original_transaction': {
                    'txid': original_txid,
                    'timestamp': datetime.now().isoformat(),
                    'coinjoin_score': coinjoin_analysis.get('coinjoin_score', 0),
                    'detection_method': coinjoin_analysis.get('detection_method', 'unknown'),
                    'indicators': coinjoin_analysis.get('indicators', {}),
                    'fee': original_tx_data.get('fee', 0),
                    'size': original_tx_data.get('size', 0)
                },
                'coinjoin_addresses': list(self.coinjoin_addresses),
                'coinjoin_transactions': list(self.coinjoin_transactions),
                'related_addresses': list(investigation_results.get('related_addresses', set())),
                'related_transactions': list(investigation_results.get('related_transactions', set())),
                'investigation_stats': {
                    'depth_reached': investigation_results.get('depth', 0),
                    'addresses_processed': investigation_results.get('addresses_processed', 0),
                    'coinjoin_found': investigation_results.get('coinjoin_found', 0),
                    'normal_found': investigation_results.get('normal_found', 0)
                }
            }
            
            # Store to Neo4j
            await self.neo4j_storage.store_coinjoin_investigation(neo4j_data)
            
            logger.info(f"üíæ ƒê√£ l∆∞u k·∫øt qu·∫£ ƒëi·ªÅu tra v√†o Neo4j: {original_txid}")
            
        except Exception as e:
            logger.error(f"Error storing investigation results: {e}")
